#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Student: Zuo Yilong (1120231863)
#
# Optimizations:
# 1. 8-way block loading (distinct registers) keeps PIPE free of load-use bubbles.
# 2. Stores plus cmov-based counting happen after all loads to maximize overlap.
# 3. Remainder uses a simple loop; constants 1/0 kept in registers to avoid recalculation.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	xorq %rax,%rax
	andq %rdx,%rdx
	jle Done

	irmovq $1, %rbx		# const 1
	xorq %rbp, %rbp		# const 0

	iaddq $-12, %rdx
	jl Remainder

Loop12:
	mrmovq   (%rdi), %r8
	mrmovq  8(%rdi), %r9
	mrmovq 16(%rdi), %r10
	mrmovq 24(%rdi), %r11
	mrmovq 32(%rdi), %r12
	mrmovq 40(%rdi), %r13
	mrmovq 48(%rdi), %r14
	mrmovq 56(%rdi), %rcx

	rmmovq %r8,   (%rsi)
	andq %r8, %r8
	rrmovq %rbp, %r8
	cmovg %rbx, %r8
	addq %r8, %rax

	rmmovq %r9,  8(%rsi)
	andq %r9, %r9
	rrmovq %rbp, %r9
	cmovg %rbx, %r9
	addq %r9, %rax

	rmmovq %r10, 16(%rsi)
	andq %r10, %r10
	rrmovq %rbp, %r10
	cmovg %rbx, %r10
	addq %r10, %rax

	rmmovq %r11, 24(%rsi)
	andq %r11, %r11
	rrmovq %rbp, %r11
	cmovg %rbx, %r11
	addq %r11, %rax

	mrmovq 64(%rdi), %r8
	mrmovq 72(%rdi), %r9
	mrmovq 80(%rdi), %r10
	mrmovq 88(%rdi), %r11

	rmmovq %r12, 32(%rsi)
	andq %r12, %r12
	rrmovq %rbp, %r12
	cmovg %rbx, %r12
	addq %r12, %rax

	rmmovq %r13, 40(%rsi)
	andq %r13, %r13
	rrmovq %rbp, %r13
	cmovg %rbx, %r13
	addq %r13, %rax

	rmmovq %r14, 48(%rsi)
	andq %r14, %r14
	rrmovq %rbp, %r14
	cmovg %rbx, %r14
	addq %r14, %rax

	rmmovq %rcx, 56(%rsi)
	andq %rcx, %rcx
	rrmovq %rbp, %rcx
	cmovg %rbx, %rcx
	addq %rcx, %rax

	rmmovq %r8,  64(%rsi)
	andq %r8, %r8
	rrmovq %rbp, %r8
	cmovg %rbx, %r8
	addq %r8, %rax

	rmmovq %r9,  72(%rsi)
	andq %r9, %r9
	rrmovq %rbp, %r9
	cmovg %rbx, %r9
	addq %r9, %rax

	rmmovq %r10, 80(%rsi)
	andq %r10, %r10
	rrmovq %rbp, %r10
	cmovg %rbx, %r10
	addq %r10, %rax

	rmmovq %r11, 88(%rsi)
	andq %r11, %r11
	rrmovq %rbp, %r11
	cmovg %rbx, %r11
	addq %r11, %rax

	iaddq $96, %rdi
	iaddq $96, %rsi
	iaddq $-12, %rdx
	jge Loop12

Remainder:
	iaddq $12, %rdx		# restore remaining count
	jle Done

Tail:
	mrmovq (%rdi), %r8
	rmmovq %r8, (%rsi)
	andq %r8, %r8
	rrmovq %rbp, %r8
	cmovg %rbx, %r8
	addq %r8, %rax
	iaddq $8, %rdi
	iaddq $8, %rsi
	iaddq $-1, %rdx
	jg Tail
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad -3
	.quad 4
	.quad 5
	.quad -6
	.quad 7
	.quad 8
	.quad -9
	.quad -10
	.quad -11
	.quad -12
	.quad 13
	.quad -14
	.quad 15
	.quad 16
	.quad 17
	.quad -18
	.quad -19
	.quad 20
	.quad -21
	.quad 22
	.quad 23
	.quad 24
	.quad -25
	.quad -26
	.quad -27
	.quad -28
	.quad 29
	.quad -30
	.quad -31
	.quad 32
	.quad 33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad 38
	.quad -39
	.quad 40
	.quad -41
	.quad 42
	.quad -43
	.quad -44
	.quad 45
	.quad 46
	.quad -47
	.quad -48
	.quad -49
	.quad 50
	.quad -51
	.quad -52
	.quad -53
	.quad 54
	.quad -55
	.quad -56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
